\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}

\newtheorem{proposition}{Proposition}

\newcommand{\indep}{\perp \!\!\! \perp}
\author{M2 Graphical Models MLE team}
\title{Graphical Models MLE}
\begin{document}
\maketitle

\tableofcontents
\section{Introduction}
%In the following, we follow~\cite{lauritzen1996graphical, drton2008lectures, sullivant2018algebraic}.
Let $G=(V,E)$ be a simple graph (i.e., a graph with no loops or parallel edges), where $V$ is the set of vertices and $E$ is the set of edges. We denote by $(a,b)\in E$ a directed edge from $a$ to $b$. If $E$ contains both $(a,b)$ and $(b,a)$, then this is an undirected edge. 

In the context of algebraic statistics, the vertices of a graph are random variables. This induces a partition:
\[V=\Delta \cup \Gamma \quad \text{and} \quad \Delta \cap \Gamma=\emptyset,\]
where $\Delta$ are the discrete variables and $\Gamma$ are continuous variables. A graph is called \emph{pure}, if it contains only one type of variables.  We will assume that all our graphs are \emph{pure}.

\textcolor{blue}{Would "dirty" graphs be the next step for the package?}

If $(a,b)$ is a directed edge, then $a$ is called the \emph{parent} of $b$ and $b$ is called the \emph{child} of $a$. If $(a,b)$ is an undirected edge, then the vertices $a$ and $b$ are called \emph{adjacent} or \emph{neighbours}. More generally, if there is a directed path from $a$ to $b$ and no reverse path from $b$ to $a$ exists, then $a$ is called an \emph{ancestor} of $b$ and $b$ is called a descendant of $a$. 
Graphs that contain only directed edges are called directed (DAGs) and graphs with only undirected edges are called undirected. We are also interested in a special type of graphs that contains both types of edges called \emph{chain graphs}. If $G=(V,E)$ is a chain graph, then the set of vertices can be partitioned into
\[V=V_1 \cup V_2 \cup \cdots V_k\]
such that all edges within the same component are undirected and the edges between components are directed and the direction goes from the smaller component to the larger. 
\textcolor{blue}{TBD: illustrate with graphs (e.g., \cite[p. 6 Fig 2.2]{lauritzen1996graphical} )}

Intuitively, the edges of a graph encode the dependencies between variables. If two non-adjacent vertices do not have the same ancestor, then they are \emph{(marginally) independent}. On the other hand, if they share  ancestors, then they are \emph{conditionally independent} given the common ancestors. 

The conditional independence constraints are called the \emph{Markov properties} of a graph $G$. A \emph{graphical model} induced by $G$ is a family of probability distributions that satisfy the Markov properties.

\section{Conditional independence (CI)}\label{sec: CI}
Let $X=(X_1,X_2,\cdots,X_m)\in \mathbb{R}^m$ be a random vector taking values in the probability space $\mathcal{X}=\mathcal{X}_1\times\mathcal{X}_2\times\cdots\times\mathcal{X}_m$. In the case of a graphical model, each $X_i$ corresponds to a vertex of the graph $G$ and $V=[m]$. Let $f(X)$ be the joint probability density function of $X$ with respect to the product measure on $\nu$ on $\mathcal{X}$ and we assume that $f(X)$ is continuous.  

Let $A \subseteq [m]$ and let $X_A:=\{X_a:\quad a \in A\}$ be the subset of variables indexed by $A$. We obtain the \emph{marginal distribution} of $X_A$ by integrating out $x_{[m]-A}$
\[f_A(x_A):=\int_{\mathcal{X}_{[m]-A}} f(x)d\nu(x_{[m]-A}),\quad x \in \mathcal{X}_A.\]

Let $X_B=x_b$ and assume $f_B(x_B)>0$. Then the \emph{conditional density} of $X_A$ given $X_B=x_b$ is defined using the Bayes law
\[f_{A|B}(x_A|x_B):=\frac{f_{A\cup B}(x_A,x_B)}{f_B(x_B)}.\]

This leads to the notion of \emph{conditional independence}. Let $A,B,C$ be pairwise dijoint subsets of $[m]$. Assume $X_c=x_c$ and $f_C(x_C)>0$. Then we say that $X_A$ and $X_B$ are conditionally independent given $X_C=x_c$ if
\[f_{A\cup B|C}(x_A,x_B|x_C)=f_{A|C}(x_A|x_C)f_{B|C}(x_B|x_C).\]
We write $X_A\indep X_B|X_C$. If $C=\emptyset$, then the conditional independence is called marginal independence and we write $X_A\indep X_B$. One can also use the short-hand notation $A \indep B |C$ and $A \indep B$, respectively.

\textcolor{blue}{TBD: Add conditional independence axioms (incl. C5) \cite[p. 29]{lauritzen1996graphical} or \cite[Proposition 3.1.2 and Proposition 3.1.3]{drton2008lectures}}

Given a set of conditions, a \emph{statistical model} is the set of all probability distributions that satisfy these conditions. This includes whether the probability distribution is discrete or continuous and possible conditional independence conditions.

\subsection{Maximum likelihood estimation (MLE) and contingency tables}
Let $\mathcal{D}=\{X^{(1)},X^{(2)},\cdots,X^{(n)} \in \mathbb{R}^m\}$ be the data consisting of $n$ independent and identically distributed (i.i.d.) random vectors $X^{(i)}\sim P_\theta$. The idea of maximum likelihood estimation (MLE) is to determine which probability distribution $P_\theta$ maximizes the probability of observing a given sample. 

We describe the general procedure. We call
\[L(\theta):=\prod_{i=1}^np_\theta(X^{(i)})\]
the \emph{likelihood function}. And we assume that $\theta$ comes from a certain parameter family $\Theta$. Out goal is to find the \emph{estimate} $\hat{\theta}$ given by
\[\hat{\theta} := \underset{\theta\in\Theta}{\operatorname{arg\;max}}\ L(\theta).\]
This optimization problem is equivalent to a more convenient formulation using the \emph{log-likelikehood function}
\begin{equation}\label{log-likelihood function}
\ell(\theta):= \log L(\theta)=\sum_{i=1}^np_\theta(X^{(i)}),
\end{equation}
so we will usually take $\hat{\theta}$ as
\begin{equation}\label{optimization problem}
\hat{\theta} = \underset{\theta\in\Theta}{\operatorname{arg\;max}}\ \ell(\theta).
\end{equation}


If an optimal solution exists, it is a solution of a system of equations known as \emph{score equations} or \emph{likelihood equations}
\[\frac{\partial \ell}{\partial \theta_i}=0.\]
Given \emph{generic} data, the number of complex solutions to this system of equations is called the \emph{ML degree}.

\textcolor{blue}{Write about contingency tables when discussing discrete CI models. Move the part about saturated models here. \cite[Chapter 1]{drton2008lectures} and (especially!) \cite[p.69]{lauritzen1996graphical}}
\subsection{Discrete CI models}
Let $X=(X_1,X_2,\cdots,X_m)$ be a discrete random vector.
\textcolor{blue}{TBD. Description of discrete CI models and their properties in \cite[pp. 70-73]{drton2008lectures} and the MLE in \cite[pp. 36-43]{drton2008lectures}}
\subsection{Gaussian CI models}
Let $X=(X_1,X_2,\cdots,X_m)$ follow a multivariate normal distribution $\mathcal{N}(\mu,\Sigma)$, that is, it has the probability density function
\[p_{\mu,\Sigma}(x) = \frac{\exp\left(-\frac 1 2 ({ x}-{\mu})^\mathrm{T}{\Sigma}^{-1}({x}-{\mu})\right)}{\sqrt{(2\pi)^m|\Sigma|}}, \quad x \in \mathbb{R}^m,\]
where $\mu \in \mathbb{R}^m$ and $\Sigma$ is assumed to be positive definite. This is a special case of of continuous conditional independence models.

Under the assumption of non-degeneracy of $\Sigma$, a Gaussian model $\mathcal{P}_\Theta$
\[\mathcal{P}_\Theta=\{\mathcal{N}(\mu,\Sigma):\theta=(\mu,\Sigma)\in \Theta\}.\]
In general, the space of parameters $\Theta$ is a subset of $\mathbb{R}^m \times PD_m$. However, if there are no conditional independence conditions, then $\Theta=\mathbb{R}^m \times PD_m$ and such models are called \emph{saturated}. This is equivalent to saying the all variables are assumed to be mutually independent.

In the case of Gaussian models, the log-likelihood function~(\ref{log-likelihood function})  becomes
\begin{align}
\ell(\mu,\Sigma)&=-\frac{n}{2}\log \det \Sigma - \frac{1}{2}\operatorname{tr}\left(\Sigma^{-1}\cdot \sum_{i=1}^n(X^{(i)}-\mu)(X^{(i)}-\mu)^T\right)+C \nonumber\\
		&=-\frac{n}{2}\log \det \Sigma - \frac{n}{2}\operatorname{tr}\left(S\Sigma^{-1}\right)- \frac{n}{2}\left(\overline{X}-\mu\right)^T\Sigma^{-1}\left(\overline{X}-\mu\right)+C,
\end{align}
where $C$ is the constant term and
\[\overline{X}=\frac{1}{n}\sum_{i=1}^nX^{(i)} \quad \text{and} \quad S=\frac{1}{n}\sum_{i=1}^n(X^{(i)}-\overline{X})(X^{(i)}-\overline{X})^T.\]
The quantities $\overline{X}$ and $S$ are known as the \emph{sample mean} and \emph{sample covariance matrix}, respectively.
In the case of a saturated Gaussian model, the solution to the optimization problem~(\ref{optimization problem}) is
\[\hat{\theta}=(\hat{\mu},\hat{\Sigma})=(\overline{X},S).\]
In the more general case when $\Theta\subsetneq\mathbb{R}^m \times PD_m$, the estimate of the mean is still $\hat{\mu}=\overline{X}$ but, after omitting the constant term, the estimate for the covariance matrix is now determined by
\begin{equation*}
\ell(\Sigma)=-\frac{n}{2}\log \det \Sigma - \frac{n}{2}\operatorname{tr}\left(S\Sigma^{-1}\right)
\end{equation*}
To solve this optimization problem, it is easier to use the \emph{concentration matrix} $K:=\Sigma^{-1}$ because the fact that $\log \det K=-\log \det \Sigma$ makes it strictly convex
\begin{equation}
\ell(K)=\frac{n}{2}\log \det K - \frac{n}{2}\operatorname{tr}\left(SK\right)
\end{equation}

Assume we work with a conditional independence model. The question is how to translate the conditional independence conditions on the parameter space $\Theta$ into a constraint in the optimization problem~(\ref{optimization problem}). This is resolved in the following proposition.
\begin{proposition}[Proposition 3.1.13 from \cite{drton2008lectures}]\label{prop: CI submatrix condition}
Let $A,B,C\subseteq[m]$ be pairwise disjoint and assume  $X\sim \mathcal{N}(\mu, \Sigma)$. A matrix $\tilde{\Sigma}$ is in the statistical model given by $X_A\indep X_B|X_C$ if and only if the submatrix $\Sigma_{A\cup C,B \cup C}$ has rank $\#C$.
\end{proposition}

Proposition~\ref{prop: CI submatrix condition} implies that one can form a \emph{Gaussian conditional independence ideal} in $\mathbb{R}[\sigma_{ij}]$ corresponding to $X_A\indep X_B|X_C$
\[I_{A \indep B|C}=\left\langle (\#C+1)\times (\#C+1) \quad \text{minors of } \Sigma_{A\cup C,B \cup C} \right \rangle.\]
\textcolor{blue}{Why do we not include $\det(\Sigma_{A\cup C,B \cup C})x-1$ in the list of generators?}

From here, given a collection $\mathcal{C}$ of linear independence conditions 
\[\mathcal{C}=\{X_{A_1}\indep X_{B_1}|X_{C_1},X_{A_2}\indep X_{B_2}|X_{C_2},\cdots\},\]
the corresponding Gaussian conditional independence ideal is given by
\[I_\mathcal{C}=I_{A_1\indep B_1|C_1}+I_{A_2\indep B_2|C_2}+\cdots.\]


\section{Graphical models}
In general, CI models do not admit a polynomial parametrization, i.e., are not unirational. The benefit of graphical CI models is that they often admit a parametrization. Furthermore, if it exists, it is given  by the combinatorial properties of the underlying graph.

\textcolor{blue}{Markov properties \cite[Section 3.2]{lauritzen1996graphical}}

There are three main types of graphical models:
\begin{enumerate}
\item undirected graphical models (also known as Markov random fields) 
\item directed graphical models (also known as Bayesian networks)
\item chain graph models 
\end{enumerate}
The chain graph models can be used to model the distributions with both discrete and continuous variables~\cite[Chapter 6.1.1]{lauritzen1996graphical}.

\textcolor{blue}{What do we mean by mixed models? }

\textcolor{blue}{Possible to-do: write a more detailed description of each type of graphical models (per \cite[Chapter 3]{drton2008lectures}) but lower priorities because they mostly connect the combinatorics of graphs with the MLE estimation and we are not doing that in the package.}





\bibliographystyle{plain}
\bibliography{references}

\end{document}